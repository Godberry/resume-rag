"""Streamlit app for AI assistant chatbot based on resume."""
import os
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import (
            create_stuff_documents_chain,
        )
from langchain_classic.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")

MY_NAME = "è¨±çš“ç¿”"
st.title(f"ğŸ¤– èˆ‡ {MY_NAME} çš„ AI åˆ†èº«èŠå¤©")
st.caption("æ‚¨å¯ä»¥å•æˆ‘é—œæ–¼å·¥ä½œç¶“æ­·ã€æŠ€èƒ½æˆ–å°ˆæ¡ˆçš„ç´°ç¯€ï¼")

# 1. è¼‰å…¥å·²ç¶“å»ºç«‹å¥½çš„å‘é‡è³‡æ–™åº«
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))
)

# 2. è¨­å®šæª¢ç´¢å™¨èˆ‡ LLM
retriever = vector_store.as_retriever(search_kwargs={"k": 3})
llm = ChatOpenAI(api_key=API_KEY, model_name="gpt-4.1-mini")

# 3. è¨­å®š Prompt (äººè¨­éå¸¸é‡è¦ï¼)
# ä½¿ç”¨ä¸€èˆ¬å­—ä¸²è€Œé f-stringï¼Œä¿ç•™ {context} èˆ‡ {question} ä¾› PromptTemplate è§£æ
template = """
ä½ æ˜¯ {MY_NAME}ã€‚è«‹æ ¹æ“šåº•ä¸‹çš„è³‡è¨Šå›ç­”é¢è©¦å®˜çš„å•é¡Œã€‚
å¦‚æœè³‡è¨Šä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èª å¯¦å›ç­”ã€Œé€™åœ¨å±¥æ­·ä¸­æ²’æœ‰æåˆ°ï¼Œä½†æˆ‘å¯ä»¥è£œå……...ã€
è«‹ä¿æŒå°ˆæ¥­ã€è‡ªä¿¡ä¸”å‹å–„çš„èªæ°£ã€‚

ç›¸é—œå±¥æ­·è³‡è¨Šï¼š
{context}

é¢è©¦å®˜å•é¡Œï¼š
{input}
"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template).partial(MY_NAME=MY_NAME)

combine_docs_chain = create_stuff_documents_chain(
    llm, QA_CHAIN_PROMPT
)

qa_chain = create_retrieval_chain(retriever, combine_docs_chain)

# 4. Streamlit èŠå¤©ä»‹é¢é‚è¼¯
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹å•æ‚¨æœ€æ“…é•·çš„æŠ€è¡“æ˜¯ä»€éº¼ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = qa_chain.invoke({"input": prompt})
        
        st.markdown(response["answer"]) 
        st.session_state.messages.append(
            {"role": "assistant", "content": response["answer"]}
        )
